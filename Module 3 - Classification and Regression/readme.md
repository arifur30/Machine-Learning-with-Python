
# Supervised Machine Learning Models – IBM ML with Python (Coursera)

This repository contains my learning notes, code, and exercises for the **Supervised Machine Learning Models** module from the **Machine Learning with Python** course by IBM on Coursera.

In this module, I explored the foundations and implementation of **modern supervised machine learning models**, focusing on both **classification** and **regression** techniques. I learned how to construct multiclass classifiers, work with decision trees and regression trees, and implement powerful models like **k-NN**, **SVM**, **Random Forest**, and **XGBoost** using Python libraries such as **Scikit-Learn** and **Snap ML**.

---

## 🚀 What I Learned

### 🔹 Core Concepts
- Understanding the difference between **classification** and **regression**
- How **binary classification** works and how to build **multiclass classifiers** from binary models
- What **decision trees** are, how they are built, and how they learn
- What **regression trees** are and how they differ from classification trees
- Exploring **bias and variance**, and how they affect model accuracy

### 🔹 Algorithms Covered
- **Decision Trees** (for both classification and regression)
- **k-Nearest Neighbors (k-NN)**
- **Support Vector Machines (SVM)**
- **Random Forest**
- **XGBoost**

### 🔹 Techniques and Concepts
- Multiclass classification: **One-vs-All**, **One-vs-One**, and **Softmax Regression**
- Evaluating models and understanding the **bias-variance tradeoff**
- Mitigating bias and variance using **bagging** and **boosting**
- Model training and inference using:
  - **Scikit-Learn**
  - **Snap ML**

---

## 📌 Learning Objectives

- Describe classification, its applications, and use cases  
- List and explain common classification algorithms (e.g., Decision Trees, k-NN)  
- Explain multiclass prediction strategies (One-vs-All, One-vs-One, Softmax Regression)  
- Build and train classification and regression trees  
- Perform data preprocessing for both classification and regression tasks  
- Train and evaluate models using **Decision Tree**, **k-NN**, **SVM**, **Random Forest**, and **XGBoost**  
- Compare training performance using **Scikit-Learn** and **Snap ML**  
- Analyze the impact of **bias and variance** in model fitting  
- Use **bagging** and **boosting** to reduce variance and improve accuracy  

---

## 🛠️ Tools and Libraries Used

- Python  
- Scikit-Learn  
- Snap ML  
- NumPy  
- Pandas  
- Matplotlib / Seaborn (for visualizations)

---



## ✅ Status

✔️ Module Completed  
📅 Course: *Machine Learning with Python by IBM*  
🏁 This repository is part of my journey to build strong foundational ML knowledge.

---

